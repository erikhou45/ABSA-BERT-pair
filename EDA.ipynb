{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "In this EDA notebook. We examine the properties of the data in order to:\n",
    "1. Evaluate if the NLI_M model proposed by Sun et al. can be used. \n",
    "2. Establish the Naive Baseline by:\n",
    "    * Calculating the average number aspect categories associated with a review\n",
    "    * The most common aspect categories\n",
    "    * The average probabilty weights of different sentiments by aspect categories\n",
    "3. Explore label proportion when applying different sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "import tokenization\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Examine the Review Lengths after Tokenization\n",
    "\n",
    "In order to use the NLI_M model proposed by Sun et al. We will need to make sure most review can fit into the sequence length of 512 after being tokenized by into Wordpiece tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up\n",
    "vocab_file = \"multi_cased_L-12_H-768_A-12/vocab.txt\"\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=vocab_file, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import tokenization\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the .xml file and take a look at the sequence lengths at review-level\n",
    "\n",
    "data_dir='./data/semeval2016/bert-pair/text-level/'\n",
    "file_name = 'EN_Laptop_Text_Train_Complete_NLI_M.csv'\n",
    "\n",
    "# load file\n",
    "with open(data_dir+file_name,\"r\",encoding=\"utf-8\") as f:\n",
    "    s=f.readline().strip()\n",
    "    examples = []\n",
    "    while s:\n",
    "        cols = s.split(\"\\t\")\n",
    "        rid = cols[0]\n",
    "        text_a = cols[2]\n",
    "        text_b = cols[3]\n",
    "        examples.append((rid,text_b))\n",
    "        s=f.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76230\n"
     ]
    }
   ],
   "source": [
    "# Check number of records\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76230/76230 [01:12<00:00, 1057.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Transform in the wordpiece tokens and calculate sequence lengths\n",
    "from tqdm import tqdm\n",
    "lengths = []\n",
    "for rid, text in tqdm(examples):\n",
    "    lengths.append(len(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([25740., 23760., 10296.,  5544.,  3762.,  3366.,  1386.,   990.,\n",
       "          990.,   396.]),\n",
       " array([  9.,  55., 101., 147., 193., 239., 285., 331., 377., 423., 469.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a histogram\n",
    "plt.hist(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=76230, minmax=(9, 469), mean=104.01298701298701, variance=7295.900735190769, skewness=1.6223863456381131, kurtosis=2.571300071849702)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe stats\n",
    "stats.describe(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above diagram and stats, we know that the reviews are short enough to fit into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Establish Naive Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Entity-Aspect Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we examine the training data file, `EN_Laptop_Train_Complete_NLI_M.csv` (the complete training set) to determine the baseline to use for our metrics. Ideally, our cross-lingual model should perform better than the baseline to show that training using English data will help the model to perform the same task in Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data/semeval2016/bert-pair/text-level/'\n",
    "\n",
    "file_name = 'EN_Laptop_Text_Train_Complete'\n",
    "task_name = 'NLI_M'\n",
    "\n",
    "train_df = pd.read_csv(data_dir+file_name+\"_\"+task_name+\".csv\", sep = \"\\t\", names = [\"sent_id\", \"polarity\", \"entity-aspect\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    385.000000\n",
       "mean       5.200000\n",
       "std        2.423625\n",
       "min        1.000000\n",
       "25%        3.000000\n",
       "50%        5.000000\n",
       "75%        6.000000\n",
       "max       14.000000\n",
       "Name: quant_pol, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a series with sentence as id and the number of entity-aspects associated with as value and summarize the stats\n",
    "train_df[\"quant_pol\"] = train_df.polarity.map({'positive':1, 'neutral':1, 'negative':1, 'conflict':1,'none':0})\n",
    "train_df.groupby(by = [\"sent_id\"]).quant_pol.sum().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entity-aspect</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LAPTOP-GENERAL</th>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAPTOP-OPERATION_PERFORMANCE</th>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAPTOP-DESIGN_FEATURES</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAPTOP-QUALITY</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAPTOP-PRICE</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              count\n",
       "entity-aspect                      \n",
       "LAPTOP-GENERAL                  385\n",
       "LAPTOP-OPERATION_PERFORMANCE    190\n",
       "LAPTOP-DESIGN_FEATURES          150\n",
       "LAPTOP-QUALITY                  148\n",
       "LAPTOP-PRICE                    114"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the top 5 most common entity-aspect combo\n",
    "count_df = train_df.loc[train_df[\"polarity\"] != \"none\", [\"sent_id\", \"entity-aspect\"]] \\\n",
    "                   .groupby(by=\"entity-aspect\") \\\n",
    "                   .agg(count = (\"sent_id\", \"count\"))\n",
    "count_df.sort_values(by = \"count\", ascending=False) \\\n",
    "        .head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above EDA, we see that both the average and median number of aspect categories associated with a review is closest to five. A reasonable baseline could be using the just the most common 5 aspect categories above as the guesses for the entity-aspect classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 How we stablish naive baseline for sentiment classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment classification task evaluates the sentiments for different entity-aspect combination separately. Therefore, since we have 198 combinations, we will need 198 different sentiments as baselines for the entity-aspect combos respectively. \n",
    "\n",
    "In addition, we will calculate the probability weights of different sentiments for a given aspect category by counting the number of reviews associated with that particular aspect category and sentiment and then normalize. So we can use the resulting weights to calculate 3 class and 2 class accuracies. \n",
    "\n",
    "Lastly, we will also use the most common sentiments in the training set as prediction for entity-aspect combination that exists in test set but not in the training set.\n",
    "\n",
    "Acceptable sentiments are non-none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Make Baseline Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"./data/semeval2016/bert-pair/text-level/EN_Laptop_Text_Test_Gold_NLI_M.csv\"\n",
    "df_test = pd.read_csv(test_file, \n",
    "                     delimiter = \"\\t\",\n",
    "                     names = [\"sent_id\", \"label\", \"entity-aspect\", \"text\"]\n",
    "                     )\n",
    "map_dict = {'positive':0, 'neutral':1, 'negative':2, 'conflict':3, 'none':4}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the probability weights by counting reviews in the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_by_aspect = train_df.loc[:,[\"sent_id\", \"polarity\", \"entity-aspect\"]] \\\n",
    "                        .groupby(by = [\"entity-aspect\", \"polarity\"]) \\\n",
    "                        .agg(count = (\"sent_id\", \"count\")) \\\n",
    "                        .unstack(fill_value = 0)\n",
    "pol_by_aspect.columns = pol_by_aspect.columns.droplevel()\n",
    "pol_by_aspect = pol_by_aspect[[\"positive\", \"neutral\", \"negative\", \"conflict\", \"none\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_probs = pol_by_aspect.divide(pol_by_aspect.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the sentiment prediction and probability weights for the top 5 aspect categories identified in Section 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_probs.insert(loc = 0, column = \"pred\", value = \"none\")\n",
    "baseline_probs.loc[\"LAPTOP-GENERAL\", :] = [\"positive\", 1.0, 0.0, 0.0, 0.0, 0.0]\n",
    "baseline_probs.loc[\"LAPTOP-OPERATION_PERFORMANCE\", :] = [\"positive\", 1.0, 0.0, 0.0, 0.0, 0.0]\n",
    "baseline_probs.loc[\"LAPTOP-DESIGN_FEATURES\", :] = [\"positive\", 1.0, 0.0, 0.0, 0.0, 0.0]\n",
    "baseline_probs.loc[\"LAPTOP-PRICE\", :] = [\"positive\", 1.0, 0.0, 0.0, 0.0, 0.0]\n",
    "baseline_probs.loc[\"LAPTOP-QUALITY\", :] = [\"negative\", 0.0, 0.0, 1.0, 0.0, 0.0]\n",
    "baseline_probs[\"pred\"] = baseline_probs.pred.map(map_dict)\n",
    "baseline_probs = baseline_probs.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline_df = df_test.reset_index().merge(baseline_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline_df = test_baseline_df.sort_values(by=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to SAVE THE TEST BASELINE FILES\n",
    "# for i in range(6):\n",
    "#     test_baseline_df.iloc[:,-6:].to_csv(f\"results/semeval2016/text-level/Test_Baseline/test_ep_{i+1}.txt\", sep = \" \", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Sampling Strategy Exploration\n",
    "\n",
    "In this section, we listed the sampling strategies used in the projects and the resulting label proportion in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data/semeval2016/bert-pair/text-level/'\n",
    "\n",
    "file_name = 'EN_Laptop_Text_Train'\n",
    "task_name = 'NLI_M'\n",
    "\n",
    "df_train = pd.read_csv(data_dir+file_name+\"_\"+task_name+\".csv\", sep = \"\\t\", names = [\"sent_id\", \"label\", \"entity-aspect\", \"text\"])\n",
    "map_dict = {'positive':0, 'neutral':1, 'negative':2, 'conflict':3, 'none':4}\n",
    "df_train[\"label\"] = df_train.label.map(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combo-Samping-1\n",
      "label\n",
      "0    0.054508\n",
      "1    0.006207\n",
      "2    0.033676\n",
      "3    0.001905\n",
      "4    0.903704\n",
      "Name: sent_id, dtype: float64\n",
      "========================================\n",
      "Combo-Samping-2\n",
      "label\n",
      "0    0.085367\n",
      "1    0.009720\n",
      "2    0.052741\n",
      "3    0.002984\n",
      "4    0.849188\n",
      "Name: sent_id, dtype: float64\n",
      "========================================\n",
      "Combo-Samping-3\n",
      "label\n",
      "0    0.141403\n",
      "1    0.016101\n",
      "2    0.087361\n",
      "3    0.004942\n",
      "4    0.750193\n",
      "Name: sent_id, dtype: float64\n",
      "========================================\n",
      "Over-Sampling-1\n",
      "label\n",
      "0    0.028633\n",
      "1    0.003260\n",
      "2    0.017690\n",
      "3    0.001001\n",
      "4    0.949417\n",
      "Name: sent_id, dtype: float64\n",
      "========================================\n",
      "Over-Sampling-2\n",
      "label\n",
      "0    0.021523\n",
      "1    0.004902\n",
      "2    0.019503\n",
      "3    0.002507\n",
      "4    0.951565\n",
      "Name: sent_id, dtype: float64\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "#Iteration 2, Model Name: Combo-Samping-1\n",
    "#Duplicate each non-none example by a factor of 2\n",
    "#Randomly drop 50 percent of none examples\n",
    "count_by_2016 = df_train.groupby(by=\"label\").sent_id.count()\n",
    "count_by_2016.iloc[:4] = count_by_2016.iloc[:4]*2\n",
    "count_by_2016.iloc[4] = count_by_2016.iloc[4]*0.5\n",
    "print(\"Combo-Samping-1\")\n",
    "print(count_by_2016/count_by_2016.sum())\n",
    "print(\"=\"*40)\n",
    "\n",
    "#Iteration 2, Model Name: Combo-Samping-2\n",
    "#Duplicate each non-none example by a factor of 3\n",
    "#Randomly drop 0.55 percent of none examples\n",
    "count_by_2016 = df_train.groupby(by=\"label\").sent_id.count()\n",
    "count_by_2016.iloc[:4] = count_by_2016.iloc[:4]*3\n",
    "count_by_2016.iloc[4] = count_by_2016.iloc[4]*0.45\n",
    "print(\"Combo-Samping-2\")\n",
    "print(count_by_2016/count_by_2016.sum())\n",
    "print(\"=\"*40)\n",
    "\n",
    "#Iteration 2, Model Name: Combo-Samping-3\n",
    "#Duplicate each non-none example by a factor of 5\n",
    "#Randomly drop 60 percent of none examples\n",
    "count_by_2016 = df_train.groupby(by=\"label\").sent_id.count()\n",
    "count_by_2016.iloc[:4] = count_by_2016.iloc[:4]*5\n",
    "count_by_2016.iloc[4] = count_by_2016.iloc[4]*0.4\n",
    "print(\"Combo-Samping-3\")\n",
    "print(count_by_2016/count_by_2016.sum())\n",
    "print(\"=\"*40)\n",
    "\n",
    "#Iteration 3, Model Name: Over-Sampling-1\n",
    "#Duplicate each non-none example by a factor of 2\n",
    "count_by_2016 = df_train.groupby(by=\"label\").sent_id.count()\n",
    "count_by_2016.iloc[:4] = count_by_2016.iloc[:4]*2\n",
    "print(\"Over-Sampling-1\")\n",
    "print(count_by_2016/count_by_2016.sum())\n",
    "print(\"=\"*40)\n",
    "\n",
    "#Iteration 3, Model Name: Over-Sampling-2\n",
    "#Duplicate\n",
    "#positive example by a factor of 1.5\n",
    "#neutral example by a factor of 3\n",
    "#negative example by a factor of 2.2\n",
    "#conflict example by a factor of 5\n",
    "count_by_2016 = df_train.groupby(by=\"label\").sent_id.count()\n",
    "count_by_2016.iloc[0] = count_by_2016.iloc[0]*1.5\n",
    "count_by_2016.iloc[1] = count_by_2016.iloc[1]*3\n",
    "count_by_2016.iloc[2] = count_by_2016.iloc[2]*2.2\n",
    "count_by_2016.iloc[3] = count_by_2016.iloc[3]*5\n",
    "print(\"Over-Sampling-2\")\n",
    "print(count_by_2016/count_by_2016.sum())\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
